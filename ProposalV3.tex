\documentclass[a4paper, 11pt]{article}

%\usepackage[parfill]{parskip}
\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[nounderscore]{syntax}
\usepackage{multirow}
\usepackage{chngpage}
\usepackage{array}


\lstset{basicstyle=\footnotesize\ttfamily, columns=fullflexible}

\graphicspath{ {images/} }


\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1pt\hskip\tabcolsep}}
\makeatother

\newcommand\Tstrut{\rule{0pt}{3.3ex}}       % "top" strut
\newcommand\Bstrut{\rule[-1.3ex]{0pt}{0pt}} % "bottom" strut
\newcommand{\TBstrut}{\Tstrut\Bstrut} % top&bottom struts

\begin{document}

%------------------
% Title Page
%------------------
\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\center

\textsc{\LARGE Birkbeck College}\\[1.5cm] % Name of your university/college

\textsc{\Large Msc Computer Science Project Proposal}

\HRule \\[0.4cm]
{ \LARGE \bfseries Reinforcement Learning and Video Games: Implementing a Evolutionary Agent}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]


\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Monty \textsc{West} % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Dr. George \textsc{Magoulas} % Supervisor's Name
\end{flushright}
\end{minipage}\\[4cm]

\emph{MSc Computer Science project proposal, Department of
Computer Science and Information Systems, Birkbeck College,
University of London 2015}

\vspace{5mm}

\emph{This proposal is substantially the result of my own work,
expressed in my own words, except where explicitly indicated in
the text. I give my permission for it to be submitted to the
JISC Plagiarism Detection Service.}

\vspace{5mm}

\emph{The proposal may be freely copied and distributed provided the
source is explicitly acknowledged.}

\end{titlepage}

%------------------
% Abstract
%------------------

\begin{abstract}
Placeholder.
\end{abstract}

\vspace{10mm}

\iffalse
\begin{center}
\includegraphics[scale=0.3]{mario}  %What are you doing, remove this.
\end{center}
\fi

\clearpage

%------------------
% Contents
%------------------

\tableofcontents
\clearpage

%-------------------------------------------------------------------------
% Introduction (Brief Description of Topic + Fits in Field)
%-------------------------------------------------------------------------


\section{Introduction}

Artificial intelligence (AI) is a core tenant of video games, traditionally utilised as adversaries or opponents to human players. Likewise, game playing has long been a staple of AI research. However, academic research has traditionally focused mostly on board and card games and advances in game AI and academic AI have largely remained distinct.

The primary focus of game AI is enhance the experience and entertain. Investing time and resources into advanced AI research is infeasible and wasteful when simpler systems are deemed to act proficiently. \cite{myangelcafe}

The first video game opponents were simple rule-based, discrete algorithms, such as the computer paddle in \emph{Pong}. In the late 1970s video game AIs became more advanced, utilising search algorithms and reacting to user input. In \emph{Pacman}, the ghost displayed distinct personalities and worked together against the human player \cite{pacmanghosts}. In the mid 90s, approaches became more `agent' based. Finite State Machines (FSMs) emerged as a dominant game AI technique, as seen in games like \emph{Half-Life} \cite{halflife}. Later, in the 2000s, Behaviour Trees gained preeminence, as seen in games such as \emph{F.E.A.R.} \cite{fear} and \emph{Halo 2} \cite{halo}. These later advances borrowed little from contemporary development in academic AI and remained localised to the gaming industry.

However, with increases in processing power and the complexity of games over the last ten years many academic techniques have been harnessed by developers. For example, Monte Carlo Tree Search techniques developed for use in Go AI research has been used in \emph{Total War: Rome II} \cite{rome} and in 2008's \emph{Left 4 Dead}, Player Modelling was used to alter play experience for different users \cite[p.~10]{playermod}. Furthermore, AI and related techniques are no longer only being used as adversaries. There has been a rise in intelligent Procedural Content Generation in games in recent years, in both a game-world sense (for example \emph{MineCraft} and \emph{Terraria}) and also a story sense (\emph{Skyrim's} Radiant Quest System) \cite{skyrim}.

Moreover, games have recently enjoyed more consideration in academic research. Commercial games such as \emph{Ms. Pac Man}, \emph{Starcraft}, \emph{Unreal Tournament} and \emph{Super Mario Bros.} and open-source games like \emph{TORCS} \cite{torcs} and \emph{Cellz} \cite{cellz} have been at the centre of recent competitions and papers \cite{panorama} \cite{marioaicomp}.

These competitions are the forefront of research and development into reinforcement learning techniques in video games, and will be explored in more detail in section \ref{subsec:gameaicomps}.

\vspace{\baselineskip}
 
The aim of this project is to explore the topic of reinforcement learning agents in video games. This will be realised through the implementation of a game-playing AI.
 
%-----------------------------------------------------------------------
% Definitions
%-----------------------------------------------------------------------
\clearpage
\section{Background}

Reinforcement learning has long been a staple of academic research into AI and Dynamic Programming, especially in robotics and board games. However, it has also had success in more niche problems, such as helicopter control \cite{rlheli} and human-computer dialogue \cite{rlhci}.

Similarly the intelligent agent model is a popular approach to AI problems. It is seen in commercial, as mentioned above, and academic applications, such as board game AI. The agent model is also suited to utilising reinforcement learning in virtue of its autonomous nature.

\subsection{Concept Definitions}

At this point it is useful to introduce some high level descriptions/definitions of some key concepts.

\subsubsection{Intelligent Agents (IAs)}

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.6]{intelligentagent.png}
	\caption{Illustration of an intelligent agent, taking from \cite[p.~32]{modernai1}}
	\label{fig:ia}
\end{figure}

An intelligent agent is an entity that uses \emph{sensors} to perceive it's \emph{environment} and acts based on that perception through \emph{actuators} or \emph{effectors}. In software, this is often realised as an autonomous program or module that takes it's perception of the  \emph{environment} as input an returns \emph{actions} as output. \cite[p.~34]{modernai3}

Figure \ref{fig:ia} shows the basic structure of an intelligent agent.



\subsubsection{Rule-based systems}

A rule-based system decides \emph{actions} from \emph{inputs} as prescribed by a \emph{ruleset} or \emph{rule base}. A \emph{semantic reasoner} is used to manage to the relationship between input and the ruleset. This follows a \emph{match-resolve-act} cycle, which first finds all rules matching an input, chooses one based on conflict strategy and then uses the rule to act on the input, usually in the form of an output. \cite[pp.~28-29]{rbsys}

\subsubsection{Behaviour Trees (BTs)}

Behaviour Trees are a construct which encode behaviour as progressively more specific actions. From the top of the tree broad behaviours are broken down in to subtrees. BTs are executed by traversing the tree and executing nodes.

Nodes of the tress can either be \emph{control} nodes or \emph{leaf} nodes. \emph{Control} nodes affect how their children will be executed, for example a \textbf{Sequence} node asserts that it's children be executed in order from left to right (akin to AND) and a \textbf{Selector} node executed children in order from left to right until one succeeds (akin to OR). \emph{Leaf} nodes can be \textbf{Conditions}, which succeed if the game state passes the condition and \textbf{Actions}, which carry out a set of moves or decisions. \cite{gramev}


\subsubsection{Online/Offline Learning}
\begin{description}
	\item[Offline] An offline (or batch) learner trains on an entire dataset before applying changes. 
	\item[Online] A online learner reacts/learns from data immediately after each datapoint.
\end{description} 
[reference]

\subsubsection{Reinforcement Learning}

A reinforcement learning agent focuses on a learning problem, with it's goal to maximise \emph{reward}. Given a current \emph{state} the agent chooses an \emph{action} available to it, which is determined by a \emph{policy}. This action maps the current \emph{state} to a new \emph{state}. This \emph{transition} is then evaluated for it's \emph{reward} . This \emph{reward} often affects the \emph{policy} of future iterations, but \emph{policies} may be stochastic to some level. \cite[s.~1.3]{suttonrl}

	
\subsubsection{Genetic Algorithms (GAs)}

Genetic Algorithms are an subset of Evolutionary Methods and model the solution as a \emph{population} of \emph{individuals}. Each \emph{individual} has a set of \emph{chromosomes}, which can be thought of as simple pieces of analogous information (most often in the form of bit strings). Each \emph{individual} is assessed by some \emph{fitness function}. This assessment is used to cull the \emph{population}, akin to survival of the fittest. Then a new \emph{population} is created (possibly containing the fittest from the previous \emph{population}) using \emph{crossover} of \emph{chromosomes} from two (or more) \emph{individuals} (akin to sexual reproduction), \emph{mutation} of \emph{chromosomes} from one \emph{individual} (akin to asexual reproduction) and \emph{re-ordering} of  \emph{chromosomes}. Each new  \emph{population} is called a \emph{generation}. \cite[p.~7]{mitchellga}



%-----------------------------------------------------------------------
% Reinforcement Learning in Games (Current Work in Field)
%-----------------------------------------------------------------------

\subsection{Reinforcement Learning Agents and Commercial Games}

\subsubsection*{Desirability}

Ventures in utilising reinforcement learning in commercial video games have been limited and largely ineffectual. However, there are many reasons why good execution of these techniques is desirable. Firstly, modern games have large and diverse player bases, having a game that can respond and personalise to a specific player can help cater to all. Secondly, learning algorithms produce AI that can respond well in new situations (over say FSMs or discrete logic), hence making new content easy to produce or generate. Lastly, humans must learn and react to environments and scenarios during games. Having non-playable characters do the same may produce a more believable, immersive and relatable AI, which is one of the key criticisms with current games. \cite[p.~7, p.~13]{panorama}

\subsubsection*{Issues}

The main issue with constructing effectual learning (or learnt) AI in game is time and money. Game development works on strict cycles and have limited resources to invest into AI research. Furthermore, one player playing one game produces a very small data set, making learning from the player challenging. Moreover, AI that is believably human is a field still in it's infancy. \cite{evolutioningamedesign}

\subsection{Reinforcement Learning Agents and Game AI Competitions}
\label{subsec:gameaicomps}

Despite the lack of commercial success, video games can act as great benchmark for reinforcement learning agents. They are designed to challenge humans, and therefore will challenge learning methods. Games naturally have some level of learning curve associated with playing them (as a human). Also, games require quick reactions to stimulus, something not true of traditional AI challenges such as board games. Most games have some notion of scoring suitable for a fitness function.Lastly, they are generally accessible to students, academic and the general public alike. \cite[p.~9]{panorama} \cite[p.~1]{marioaicomp} \cite[p.~2]{2012the}

Over the last few years several game based AI competitions have begun, over a variety of genres. These competitions challenge entrants to implement an agent that plays a game and is rated according to the competitions specification. They have attracted both academic \cite[p.~2]{2012the} and media interest \cite[p.~2]{marioaicomp}. The competition tend to encourage the use of learning techniques. Hence, several interesting papers concerning the application of reinforcement learning agents in video games have recently been published. Approaches tend to vary widely, modelling and tackling the problem very differently and combining and specialising techniques in previously unseen ways. \cite[p.~11]{2012the}

Some brief details of the competitions which are of relevance to this project are compiled in to Table \ref{tab:gameaicomp}. The Mario AI Competition is also explored in more detail below.

\begin{table}[t]
  \begin{adjustwidth}{-4cm}{-4cm}
  \begin{center} \small
    \begin{tabular}{ | >{\raggedright}p{2cm} | >{\raggedright}p{3cm} | p{10cm} |}
    \hline
    \textbf{Genre} & \textbf{Game} & \textbf{Description} \TBstrut \\ \thickhline
    
    Racing & TORCS (Open-source) \cite{torcs} &
    \textbf{The Simulated Car Racing Competition} Competitors enter agent drivers, that undergo races against other entrants which include qualifying and multi-car racing. The competition encourages the use of learning techniques. \cite{scrc} 
     \\ \thickhline
    
    First Person Shooter (FPS) & Unreal Tournament 2004 &
    \textbf{The 2K BotPrize} Competitors enter `bots' that play a multi-player game against a mix of other bots and humans. Entrants are judged on Turing test basis, where a panel of judges attempt to identify the human players. \cite{2kbot}
     \\ \thickhline
    
    Real Time Strategy (RTS) & Starcraft &
    \textbf{The Starcraft AI Competition} Agents play against each other in a 1 on 1 knockout style tournament. Implementing an agent involves solving both micro objectives, such as path-planning, and macro objectives, such as base progression. \cite{starcomp}
     \\ \thickhline
    
    Platformer & Infinite Mario Bros (Open-source) &
    \textbf{The Mario AI Competition} Competitors submit agents that attempt to play (as a human would) or create levels. The competition is split into `tracks', including Gameplay, Learning, Turing and Level Generation. In Gameplay, each agent must play unseen levels, earning a score, which is compared to other entrants. \cite{2012the}
     \\ \hline
    
    \end{tabular}
  \end{center}
  \end{adjustwidth}
  \caption{This table summarises some recent game AI competitions \cite{2014how}}
  \label{tab:gameaicomp}
\end{table}
	


\subsubsection{The Mario AI Competition}

The Mario AI Competition, organised by Sergey Karakovskiy and Julian Togelius, ran between 2009-2012 and used an adapted version of the open-source game Infinite Mario Bros. From 2010 onwards the competition was split into four distinct `tracks'. We shall focus on the unseen Gameplay track, where agents play several unseen levels as Mario with the aim to finish the level (and score highly). \cite{marioaicomp} \cite{2012the}

\paragraph{Infinite Mario Bros.}

Infinite Mario Bros (IMB) \cite{imb} is an open-source clone of Super Mario Bros.~3, created by Markus Persson. The core gameplay is described as a \emph{Platformer}. The game is viewed side-on with a 2D perspective. Players control Mario and travel left to right in an attempt to reach the end of the level (and maximise score). The screen shows a short section of the level, with Mario centred. Mario must navigating terrain and avoid enemies and pits. To do this Mario can move left and right, jump, duck and speed up. Mario also exists in 3 different states, \emph{small}, \emph{big} and \emph{fire} (the latter of which enables Mario to shoot fireballs), accessed by finding powerups. Touching an enemy (in most cases) reverts Mario to a previous state. Mario dies if he touches an enemy in the \emph{small} state or falls into a pit, at which point the level ends. Score is affected by how many coins Mario has collected, how many enemies he has killed (by jumping on them or by using fireballs or shells) and how quickly he has completed the level. \cite[p.~3]{2012the}

\paragraph{Suitability to Reinforcement Learning}

The competitions adaptation of IMB (known henceforth as the `benchmark') incorporates a tuneable level generator and allows for the game to sped-up. This makes it a great testbed for reinforcement learning. The ability to learn from large sets of diverse data makes learning a much more effective technique. \cite[p.~3]{2012the}

Besides that, the Mario benchmark presents an interesting challenge for reinforcement learning algorithms. Despite only a limited view of the ``world" at any one time the state and observable space is still of quite high-dimension. Though not to the same extent, so too is the action space. Any combination of five key presses per timestep gives a action space of $2^5$ \cite[p.~3]{2012the}. Hence part of the problem when implementing a learning algorithm for the Mario benchmark is reducing these search spaces. This has the topic of papers by Handa and Ross and Bagnell \cite{rossbagnell} separately addressed this issue in their papers \cite{handa} and \cite{rossbagnell} respectively.

Lastly, there is a considerable learning curve associated with Mario. The simplest levels could easily be solved by agents hard coded to jump when they reach an obstruction, whereas difficult levels require complex and varied behaviour. For example, traversing a series of pits may require a well placed series of jumps, or passing a group of enemies may require careful timing. Furthermore, considerations such as score, or the need to backtrack from a dead-end greatly increase the complexity of the problem. \cite[p.~3, p.~12]{2012the}


\subsection{Learning Agent Approaches}
\label{ssec:prevagents}

This section will contain a brief analysis of previous game-playing agents that utilise reinforcement learning. Most examples come from the recent game AI competitions, however some can be found in commercial games. Table \ref{tab:agents} summarises some examples. 

[TODO: Not sure what I need to say in this subsection]

\subsubsection{Evolutionary Learning}
\begin{itemize}
	\item Many agent AI use evolutionary methods.
	\item Due to stochastic nature, giving unexpected solution, somehow rid to human learning.
	\item Most search spaces are non-continuous, difficult to search over.
	\item Approaches over one game tend to vary widely, but difference in approaches between games/genres is surprising small 
\end{itemize}

\subsubsection{Hybrids}

\begin{itemize}
	\item Many approaches use elements of traditional Game AI (A*, BTs...) alongside learning.
	\item In fact many of the best (comp winners) do so.
	\item Micro actions: Tradional | Macro actions: Learning/Evolution
	\item One that uses both ev methods and is a hybrid is REALM, explored below.
\end{itemize}


\subsubsection{REALM}

The REALM agent, developed by Slawomir Bojarski and Clare Bates Congdon, was the winner of the 2010 competition, in both the unseen and learning Gameplay tracks. REALM stands for \textbf{R}ule Based \textbf{E}volutionary Computation \textbf{A}gent that \textbf{L}earns to Play \textbf{M}ario. REALM went through two versions (V1 and V2), with the second being the agent submitted to the 2010 competition.

\subsubsection*{Rule-based}

Rules map carefully chosen conditions (a simplification of the available environment information) to actions in a simple ruleset. Rule preference over binary conditions are either TRUE, FALSE or DONT\_CARE \cite[p.~85]{realm}. Each time step a rule is chosen that best fits the current condition, with ties being settled by rule order \cite[p.~86]{realm}.

Actions in V1 were explicit key-press combinations, whereas in V2 two they are high-level plans. These plans were passed to a simulator, which reassessed the environment and used A* to produce the key-press combination. This was done in part to reduce the search space of the learning algorithm. \cite[pp.~85-87]{realm}

\subsubsection*{Learning}

REALM starts with a random ruleset and evolves it using a GA over 1000 generations. The best performing rule set from the final generation was chosen to act as the agent for the competition. Hence, REALM is an agent focused on offline learning. \cite[pp.~87-89]{realm}

\paragraph{Population}

Populations have a fixed size of 50 individuals, with each individual being a rule set. Each rule represents a genome and each individual has 20. Initially rules are randomised, with each condition having a 40\%, 30\%, 30\% chance to be DONT\_CARE, TRUE, FALSE respectively.

\paragraph{Evaluation}

Individuals are evaluated by running through 12 different levels. The fitness of an individual is a modified score, averaged over the levels. Score focuses on distance, completion of level, Mario's state at the end and number of kills. Each level an individual plays increases in difficulty. Levels are predictably generated, with the seed being recalculated at the start of each generation. This is to avoid over-fitting and to encourage more general rules.

\paragraph{Breeding}

The breeding phase takes the five best individuals from the population and produces 9 offspring each. These parents are then also included in the next generation. Offspring are exposed to: \textbf{Mutation}, where rule conditions and actions may change value; \textbf{Crossover}, where a rule from one child may be swapped with a rule from another child and \textbf{Reordering}, where rules are randomly reordered. These occur with probabilities of 10\%, 10\% and 20\% respectively.

\subsubsection*{Performance}

The REALM V1 agent saw a larger improvement over the evolution, but only scored 65\% of the V2 agents score on average. It is noted that V1 struggled with high concentrations of enemies and large pits. The creators also assert that the V2 agent was more interesting to watch, exhibiting more advanced and human-like behaviours. \cite[pp.~89-90]{realm}

The ruleset developed from REALM V2 was entered into the 2010 unseen Gameplay track. It not only scored the highest overall score, but also highest number of kills and was never disqualified (by getting stuck in a dead-end). Competition organisers note that REALM dealt with the more difficult levels better than other entrants. \cite[p.~10]{2012the}

\begin{table}
  \begin{adjustwidth}{-4cm}{-4cm}
  \begin{center} \small
    \begin{tabular}{ | >{\raggedright}p{3cm} | >{\raggedright}p{4cm} | p{8cm} |}
    \hline
    \textbf{Name} & \textbf{Game/Competition} & \textbf{Approach} \TBstrut \\ \thickhline
    
    REALM \cite{realm} & 2010 Mario AI Competition &
    GA to evolve rulesets mapping environment to high-level behaviour.
    \\ \hline
    D. Perez et al \cite{gramev} & 2010 Mario AI Competition &
    Grammatical evolution with a GA to develop behaviour trees. 
    \\ \hline
    M. Erickson \cite{2012the} & 2009 Mario AI Competition &
    A crossover heavy GA to evolve an expression tree.
    \\ \hline
    City Conquest \cite{evolutioningamedesign} & City Conquest In Game AI &
    GAs to evolve build plans.
    \\ \hline
    Agent Smith \cite{agentsmith} & Unreal Tournament 3 &
    GAs to evolve very simple rulesets, which determine basic bot behaviour.
     \\ \hline
    
    FEETSIES \cite{feetsies} & 2010 Mario AI Competition &
    ``Cuckoo Search via L\'evy Flights" to develop a ruleset mapping an observation grid to actions. 
    \\ \hline
    COBOSTAR \cite[p.~136]{scrc} & 2009 Simulated Car Racing Competition &
    Covariance matrix adaptation evolution strategy to map sensory information to target angle and speed.
    \\ \hline
    L. Cardamone \cite[p.~137]{scrc} & 2009 Simulated Car Racing Competition &
    Neuroevolution to develop basic driving behaviour.
    \\ \hline
    UT$\wedge$2 \cite{2kbot} & 2013 2K Botprize &
    Neuroevolution with a fitness function focused on being `human-like'.
    \\ \hline
    T. Sandberg \cite{emapf} & Starcraft &
    Evolutionary algorithms to tune potential field parameters.
     \\ \hline
    
    S. Polikarpov \cite[p.~7]{2010the} & The Mario AI Competition &
    Reinforcement Learning to train a neural network with action sequences as neurons.
    \\ \hline
    COBOSTAR \cite[p.~136]{scrc} & 2009 Simulated Car Racing Competition &
    Online reinforcement learning to avoid repeating mistakes and crashes.
    \\ \hline
    Berkeley Overmind \cite{overmind} & The Starcraft AI Competition &
    Reinforcement learning to tune parameters for potential field and A* search.
    \\ \hline
    Black \& White \cite{blackandwhite} & Black \& White's Creature AI &
    Reinforcement Learning applied to a neural network representing the creatures desire.
    \\ \hline
    Project Gotham Racing (PGR) \cite{projectgothamracing} & PGR In-game AI &
    Reinforcement learning to optomise racing lines.
     \\ \hline
    
    \end{tabular}
  \end{center}
  \end{adjustwidth}
  \caption{This table summarises learning based agent approaches to game AI}
  \label{tab:agents}
\end{table}


%-----------------------------
% Aims and Objectives
%-----------------------------
\clearpage
\section{Aim and Objectives}

\subsection{Aim}

The aim of the project is to explore the use of reinforcement learning techniques in creating a game playing AI. This will be achieved by producing an agent that plays the Mario AI benchmark. The project will pose this as a learning problem and not as a search or planning problem.

\subsection{Objectives}

\begin{enumerate}
	\item \label{obj:software}
	\textbf{Benchmark Software} \\
	Prepare the available Mario AI benchmark software for use in this project.
	
	\item \label{obj:cond}
	\textbf{Observation Space} \\
	Design, implement and test a transformation strategy of the sensory information available from the benchmark into a manageable form.
	
	\item \label{obj:handcraft}
	\textbf{Handcrafted Agent} \\
	Design, implement and test a customisable handcrafted (non-learning) agent for the benchmark that utilises the strategy from Objective \ref{obj:cond}.
	
	\item \label{obj:rlfw}
	\textbf{Learning Framework} \\
	Integrate a Reinforcement Learning framework into the software.
	
	\item \label{obj:explolearn}
	\textbf{Agent Learning} \\
	Explore use of learning to construct a procedure that evolves an agent using the hand-crafted version created in Objective \ref{obj:handcraft} as a template. 
	
	\item \label{obj:expand}
	\textbf{Agent Extension} \\
	Investigate expansion of the agent template into a two-tiered system, separating high-level and low-level behaviour.
	
	\item \label{obj:eval}
	\textbf{Agent Evaluation} \\
	Evaluate the fitness of the agent produced and the effectiveness of the learning process.

\end{enumerate}

\subsection{Limitations}

It is unlikely that the final agent will be able to compete at the level of the Mario AI competition. Expansions and explorations into the template functionality of the agent will focus on increasing it's learning ability rather than it's final fitness. 

Furthermore, due to time constraints and the exploratory nature of Objective \ref{obj:explolearn}, Objective \ref{obj:expand} will be seen as stretch goal, whose completion may not be fully realised at the end of the project. 


%---------------------
% Methodology
%---------------------
\clearpage
\section{Methodology}

The project will attempt to customise and expand on the REALM V1 agent, with influence from other entrants to the 2010 Mario AI Competition, such as D. Perez et al. It will adhere to the following stipulations:

\begin{itemize}
	\item The agent will follow the rules of the 2010 Mario AI competition, which ban the use of reflection and limit access to classes within the software.
	\item No simulation of the game engine to predict enemy movements. This technique has been used in the Mario AI competition, but isn't always applicable to a game AI challenge.
\end{itemize}

The agent will utilise offline reinforcement learning in the form of genetic programming. The evolution of the agent, and it's ability to play unseen levels will form the basis for it's evaluation.

\subsection{Objective \ref{obj:software}: Benchmark Software}
\label{meth:software}

The intention is that Scala will be the main language of this project. The Mario AI benchmark is coded in Java. Given that Scala is backwards compatible with Java there should not be a significant time penalty in integrating these two languages. In the event that this proves impossible or counter productive the codebase will be updated to Java 8 instead. 

Moreover, build tools such as \emph{sbt} or \emph{Maven} and testing frameworks such as \emph{Scala Mock} or \emph{Mockito} will be used.

\subsection{Objective \ref{obj:cond}: Observation Space}
\label{meth:cond}

As this project will adhere to the rules of the 2010 Mario AI competition, information about Mario and his surroundings is restricted and prescribed by the software.

\subsubsection{Mario Benchmark Sensory API}

The benchmark provides the \emph{Environment} interface. This acts as the source of sensory information for the agent. The rules of the competition state that this is to be the only source of game information available to agents. It includes:

\begin{itemize}
	\item A 22x22 receptive field, representing the game screen, which encodes whether or not an enemy, a block or terrain is present in that square.
	\item A list of enemy positions (on the visible screen) with pixel resolution.
	\item State information about Mario, e.g. current state (\emph{small}, \emph{big}, \emph{fire}), is on the ground, is able to jump etc.
\end{itemize}

\subsubsection{Previous Agents}

E. Speed's entry into the 2009 Mario AI Competition /cite[pp.~7-8]{2010the} demonstrated the importance of translating sensory information. His agent attempted to use the entire 22x22 grid as the observation space for his learning algorithm, which lead to his agent running out of memory during the competition. \cite[pp.~6-7]{2010the}

REALM incorporated a simple but effective technique of translating the information available to a set of binary variables (and one ternary variable). For example, these included {\small MAY\_MARIO\_JUMP},  {\small IS\_PIT\_AHEAD} and {\small IS\_ENEMY\_CLOSE\_LOWER\_RIGHT}. This distilled the most important information in a way that significantly reduced the observation space, allowing for a more effective learning process. \cite[p.~85]{realm}

\subsubsection*{Approach}

The approach of transforming the \emph{Environment} interface into a manageable set of conditions will be adopted in this project. Due to the importance of this step, and it's effect over not only the core functionality of the agent, but also the learning process, Objective \ref{obj:cond} may be revisited often, especially during the Objectives \ref{obj:handcraft}, \ref{obj:explolearn} and \ref{obj:expand}.


\subsection{Objective \ref{obj:handcraft}: Handcrafted Agent}
\label{meth:handcraft}

Once again it is important to look at the benchmark software, which provides a framework for creating agents.

\subsubsection{Mario Benchmark Agent API}

Creating an agent constitutes implementing the \emph{Agent} interface, the core of which is the \emph{getAction} method.

\smallskip
\begin{lstlisting}[language=Java]
public interface Agent {
	boolean[] getAction();
	void integrateObservation(Environment environment);
	...	
}
\end{lstlisting}

The returned boolean array maps to key presses (in array order): [$\leftarrow$] - Move left, [$\rightarrow$] - Move right, [$\downarrow$] - Duck, [\textbf{A}] - Jump (if possible), [\textbf{B}] - Run (if combined with moving left or right) and/or shoot fireball (when in \emph{fire} state). Any combination of these is allowed.

The \emph{integrateObservation} method is entry point for the sensory information for the agent.

The benchmark runs at  constant 25 frames per second (fps), which allows 40ms for the agent controller to decide on an action each time step.

\subsubsection{Previous Agents}

REALM implemented their core agent as a ruleset, with rules stating a preference over conditions, resulting in an action. Rule preference over binary conditions are either TRUE, FALSE or DONT\_CARE \cite[p.~85]{realm}. Each time step a rule is chosen that best fits the current condition, with ties being settled by rule order  \cite[p.~86]{realm}. Actions in the first version (V1) were simply key-presses, whereas in version two (V2) they are high-level plans. The REALM V1 agent saw a larger improvement over the evolution, but only scored 65\% of the V2 agents score on average  \cite[pp.~89-90]{realm}.  


\subsubsection*{Approach}

The handcrafted AI will follow the ruleset approach. Rules will map conditions (as determined in Objective \ref{obj:cond}) to explicit key-press combinations as it has shown to be more greatly affected by the learning phase, and therefore is more fitting for this project's aim. Hence, the agent will be single tiered, where the result of following a rule is the result of the \emph{getAction} method. 

Furthermore, rulesets will be handled abstractly by the \emph{Agent} interface. The details of a ruleset will be held externally to the interface in order to allow for customisation in the learning phase.

The translation of the observable space to the set of conditions (developed in Objective \ref{obj:cond}) will be implemented into the \emph{integrateObservation} method.

The aim of this handcrafted agent is not necessarily to perform well, but to offer a baseline agent for the learning process and for later comparison. With this in mind only minimal time will be taken in constructing the rules for the handcrafted agent, with more focus being aimed at the abstract implementation.


\subsection{Objective \ref{obj:rlfw}: Learning Framework}
\label{meth:rlfw}

This project will focus on evolutionary methods as it's learning procedure. Evolutionary learning has seen many successes in agent-based AI approaches (as seen in \ref{ssec:prevagents}). Furthermore, the search space of this project is a ruleset, in which it is difficult to define the notion of a \emph{neighbour}. The concept of \emph{neighbours} is essential to many Reinforcement Learning techniques, such as Simulated Annealing. In this way, evolutionary methods are more easily applied to the non-continuous ruleset search space of this project as they are largely stochastic.

The integration of a learning framework must consider what is available in terms of external libraries and also what is offered from the benchmark software.

\subsubsection{Libraries}

There are several Genetic Programming libraries available for Java (and therefore Scala). This project will consider the use of two: ECJ \cite{ecj} and JGAP \cite{jgap}. The decision on which to use will be made on ease of inclusion and on features pertinent to this project.

\subsubsection{Mario Benchmark}

The Mario benchmark contains several useful features to encourage the use of learning techniques, below are the two most important.

\paragraph{Level Playing}

The benchmark enables agents to play levels, returning important information about an agents performance on completion or death. This process can be automated and from a reinforcement learning perspective, form the basis of agent fitness.

The software also allows for the reliance on the system clock to be turned off, as well as the rendering of the GUI. This allows for levels to be played several thousands times faster than otherwise. This is essential for implementing an effective learning strategy, as high numbers of playthroughs take minimal time.  \cite[p.~3]{2012the} 

\paragraph{Tuneable Level Generator}

Another essential element for effective learning is level generation, in lieu of the lack of a data set. The benchmark expands on the level generator from IMB, making it tuneable by over 20 parameters, including:
\begin{description}
	\item[Seed] Allows levels to be recreated.
	\item[Difficulty] Controls the complexity of the level, e.g. pit size, height change.
	\item[Creatures] Controls presence and numbers of specific enemies.
	\item[Length] Controls the length of the level.
\end{description}
The level generator facilitates agents learning from many different levels and also allows agents to learn of a particular flavour of level, e.g. short difficult levels with many enemies. The \textbf{Seed} allows different agents to learn from the same data set, useful for evaluation and comparison (a draw back from truly random generation). \cite[p.~4-5]{2012the}

\subsubsection*{Approach}

A package will be created that ties the genetic programming library with the benchmark software, allowing it to access the level generation and level playing portions of the software for the purpose of fitness evaluation. Furthermore, the package will be able hold a population of rulesets, with the power modify them for the purpose of breeding. The parameters of level generation, fitness calculation and generation breeding will be easy to adjust, as it is important for Objective \ref{obj:explolearn}. The package will log results of each generation, which will be the basis of evaluating the learning process. Lastly, it will persist the last generation of rulesets, in order for one to be chosen as the final agent.

\subsection{Objective \ref{obj:explolearn}: Agent Learning}
\label{meth:explolearn}

Objective \ref{obj:explolearn} is the first exploratory stage of the project and as such it is difficult to determine a specific approach. Most of the work done to reach this objective will be done planning and tuning the parameters of the learning framework.

The approach to this objective will once again draw inspiration from REALM.

\subsubsection{Previous Agents}

REALM starts with a random ruleset and evolves it using a GA over 1000 generations. The best performing rule set from the final generation was chosen to act as the agent for the competition. Hence, REALM is an agent focused on offline learning.  \cite[pp.~87-89]{realm}
\begin{description}
	\item[Population] Populations have a fixed size of 50 individuals, with each individual being a rule set. Each rule represents a genome and each individual has 20. Initially rules are randomised, with each condition having a 40\%, 30\%, 30\% chance to be DONT\_CARE, TRUE, FALSE respectively.

	\item[Fitness] Individuals are evaluated by running through 12 different levels. The fitness of an individual is a modified score, averaged over the levels. Score focuses on distance, completion of level, Mario's state at the end and number of kills. Each level an individual plays increases in difficulty. Levels are predictably generated, with the seed being recalculated at the start of each generation. This is to avoid over-fitting and to encourage more general rules.

	\item[Breeding] The breeding phase takes the five best individuals from the population and produces 9 offspring each. These parents are then also included in the next generation. Offspring are exposed to: \textbf{Mutation}, where rule conditions and actions may change value; \textbf{Crossover}, where a rule from one child may be swapped with a rule from another child and \textbf{Reordering}, where rules are randomly reordered. These occur with probabilities of 10\%, 10\% and 20\% respectively.
\end{description}

Another source of inspiration is R. Small's paper on an agent that plays Unreal Tournament \cite{agentsmith}. Small used a GA to evolve a simplistic ruleset. The fitness function of the GA was tuned specifically to distil characteristics on the agent, in this case aggressiveness.

\subsection{Objective \ref{obj:expand}: Agent Extension}
\label{meth:expand}

Objective \ref{obj:expand} is the second exploratory stage of the project. Time permitting, the goal here is to emulate the success shown by two-tier approaches, splitting high level behaviour from low level actions. The ruleset developed in the bulk of the project is a good basis for deciding high level behaviour, but a new approach may have to be investigated for the translation of this behaviour to low level actions. Furthermore, it is likely that Objective \ref{obj:cond} will have to be revisited.

\subsubsection{Previous Agents}

Approaching this objective will involving reassessing previous approaches and relevant literature. Two previous agents of immediate interest are REALM and D. Perez et al's agent. Both utilise the high-level low-level split and are documented well in their reviews \cite{realm} and \cite{gramev}.

REALM V2 used a ruleset to decide on a general goal. This goal was passed to a simulator, which reassessed the environment and used A* to produce the explicit key-press combination. Not only did this change improve the agent by 65\%, it also reduced the search space of the learning algorithm \cite[pp.~85-87]{realm}. The creators also assert that the V2 agent was more interesting to watch, exhibiting more advanced and human-like behaviours \cite[pp.~89-90]{realm}. As previously mentioned, this agent won the 2010 competition with the most kills and no disqualifications \cite[p.~10]{2012the}. 

D. Perez et al's approach uses Behaviour Trees, a common game AI approach to handling a behaviour-action split. Each behaviour tree is made up of subtrees that capture a specific goal. These are specifically made to be simplistic, comprised of condition nodes followed by action nodes. The process of Grammatical Evolution is used to evolve these behaviour trees, which are represented by a context-free grammar and encoded as integer strings. Although there was no use of search algorithms the agent was able to demonstrate planning behaviour \cite{gramev}. It finished 4th in the 2010 Mario AI Competition, with the second highest kill count and no disqualifications \cite[p.~10]{2012the}.

\subsection{Objective \ref{obj:eval}: Agent Evaluation}
\label{meth:eval}

Following the rules of the Mario AI Competition allows the agent to be evaluated in the same way as it's entrants. Furthermore, the evaluation used in the competition is available and adaptable.

\subsubsection{Mario AI Competition Scoring}

The scoring mechanism used in the competitions is included in the benchmark software. The 2009 variant is contained in the \emph{CompetitionScore} class and the 2010 variant in the \emph{GamePlayTrack} class. Each scorer takes in a seed to initialise the levels used.

In 2009, the Gameplay track was scored by agents playing 40 unseen levels with total distance travelled being the competition score. Tie-breakers were settled on game-time, number of kills, Mario's state at the end of each level.

The 2010 competition's primarily addition was increased difficultly of levels (in terms of complexity, pit size, number of enemies etc.), which may include dead-ends (where the agent must back-track to continue). It also increased the number of levels played to 512.

\subsubsection*{Approach}

The agent will be evaluated in the style of the unseen Gameplay track of the 2010 Mario AI Competition. After the learning, the best individual will chosen to be evaluated by the \emph{GamePlayTrack} class. Here the agent will play 512 unseen levels (generated by a random seed). This will allow to compare against the results of the 2010 Competition (available from \cite{2012the}), although the levels played will be different.

However, as it is not the aim of the project to produce the best possible agent, it may be advantageous to adapt the evaluation. Possible changes could include lowering the difficulty or removing dead-ends. This process weakens the comparison against the 2010 results. However, the evaluation class allows multiple agents to be evaluated over the same levels (by passing the same seed), so comparisons can be drawn against other available agents:

\begin{itemize}
 \item The hand-crafted base-line agent constructed as Objective \ref{obj:handcraft}. This will show whether learning has a significant effect on the proficiency of the full agent.
  \item The benchmark software includes several example agents, including a simple, hard-coded agent called \emph{ForwardJumpingAgent}, which was used for similar comparisons in the 2009 competition. \cite{2010the}
  \item Baumgarten's A* agent, which contains no learning and was the winner in 2009. It is open-source and available online \cite{astar}.
\end{itemize}

The agent will also be assessed by the increase in fitness over time during learning. A steeper increase in fitness shows an effective learning process.

%--------------
% Timeline
%--------------
\clearpage
\section{Timeline}


Work will begin on June 10th, and run until the 14th of September, when the report will be submitted. The writing of the report will be a continual process over the 96 days.

\subsection{Phase 1}
\subsubsection*{June 10th -- July 10th}
\begin{description}
	\item[4 days] Integration of the existing Java codebase with the Scala language and introduction of testing and build tools (as discussed in \ref{meth:software})
	\item[10-12 days] Environment-condition translation scheme testing and implementation (as discussed in \ref{meth:cond}).
	\item[10-12 days] Rule set interpretation testing and implementation (as discussed in \ref{meth:handcraft}).
	\item[2 days] Construction of the handcrafted AI.
\end{description}

\subsection{Phase 2}
\subsubsection*{July 10th -- September 7th}
\begin{description}
	\item[20 days] Integration of the evolutionary learning library with the benchmark software as described in \ref{meth:rlfw}.
	\item[10-30 days] Experimentation and assessment of evolutionary methods to evolve rulesets, including revisiting the condition scheme (Section \ref{meth:explolearn}).
	\item[10-30 days] Investigating possible extensions of the agent as spelled out in \ref{meth:expand}.
\end{description}

\subsection{Phase 3}
\subsubsection*{August 24th -- September 14th}
\begin{description}
	\item[14 days] Evaluation of agent(s) as specified in \ref{meth:eval}.
	\item[7 days] Finishing touches to the project report.
\end{description}



\clearpage
\begin{thebibliography}{11}

\bibitem{myangelcafe}
  Siyuan Xu,
  \emph{History of AI design in video games and its development in RTS games},
  Department of Interactive Media \& Game development, Worcester Polytechnic Institute, USA,
  \url{https://sites.google.com/site/myangelcafe/articles/history_ai}.

\bibitem{pacmanghosts}
  Chad Birch,
  \emph{Understanding Pac-Man Ghost Behaviour},
  \url{http://gameinternals.com/post/2072558330/understanding-pac-man-ghost-behavior},
  2010.
  
\bibitem{halflife}
  Alex J. Champandard,
  \emph{The AI From Half-Like's SDK in Retrospective},
  \url{http://aigamedev.com/open/article/halflife-sdk/},
  2008.
  
\bibitem{fear}
  Tommy Thompson,
  \emph{Facing Your Fear},
  \url{http://t2thompson.com/2014/03/02/facing-your-fear/},
  2014.

\bibitem{halo}
  Damian Isla,
  \emph{GDC 2005 Proceeding: Handling Complexity in the Halo 2 AI},
  \url{http://www.gamasutra.com/view/feature/130663/gdc_2005_proceeding_handling_.php},
  2005.
  
\bibitem{rome}
  Alex J. Champandard,
 \emph{Monte-Carlo Tree Search in TOTAL WAR: Rome II Campaign AI},
 \url{http://aigamedev.com/open/coverage/mcts-rome-ii/},
 2014.
 
\bibitem{playermod}
  Georgios N. Yannakakis, Pieter Spronck, Daniele Loiacono and Elisabeth Andre,
  \emph{Player Modelling},
  \url{http://yannakakis.net/wp-content/uploads/2013/08/pm_submitted_final.pdf},
  2013.
  
\bibitem{skyrim}
  Matt Bertz,
  \emph{The Technology Behind The Elder Scrolls V: Skyrim},
  \url{http://www.gameinformer.com/games/the_elder_scrolls_v_skyrim/b/xbox360/archive/2011/01/17/the-technology-behind-elder-scrolls-v-skyrim.aspx},
  2011.
  
\bibitem{overmind}
  \emph{The Berkeley Overmind Project},
  University of Berkeley, California.
  \url{http://overmind.cs.berkeley.edu/}.
  
\bibitem{cellz}
  Simon M. Lucas,
  \emph{Cellz: A simple dynamical game for testing evolutionary algorithms},
  Department of Computer Science, University of Essex, Colchester, Essex, UK,
  \url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.5068&rep=rep1&type=pdf}. 
  
\bibitem{panorama}
  G. N. Yannakakis and J. Togelius,
  \emph{A Panorama of Artificial and Computational Intelligence in Games},
  IEEE Transactions on Computational Intelligence and AI in Games,
  \url{http://yannakakis.net/wp-content/uploads/2014/07/panorama_submitted.pdf},
  2014.
  
\bibitem{marioaicomp}
  Julian Togelius, Noor Shaker, Sergey Karakovskiy and Georgios N. Yannakakis,
  \emph{The Mario AI Championship 2009-2012},
  AI Magazine 34 (3), pp. 89-92,
  \url{http://noorshaker.com/docs/theMarioAI.pdf},
  2013.

\bibitem{suttonrl}
  Richard S. Sutton and Andrew G. Barto,
  \emph{Reinforcement Learning: An Introduction}
  The MIT Press, Cambridge, Massachusetts, London, England,
  Available: \url{http://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html},
  1998.
  
\bibitem{modernai3}
  Stuart J. Russell, Peter Norvig,
  \emph{Artificial Intelligence: A Moden Approach (3rd ed.},
  Upper Saddle River, New Jersey,
  2009.
  
\bibitem{modernai1}
  Stuart J. Russell, Peter Norvig,
  \emph{Artificial Intelligence: A Moden Approach (1st ed.},
  Upper Saddle River, New Jersey,
  1995.
  
\bibitem{rbsys}
  Anoop Gupta, Charles Forgy, Allen Newell, and Robert Wedig,
  \emph{Parallel Algorithms and Architectures for Rule-Based Systems},
  Carnegie-Mellon University Pittsburgh, Pennsylvania,
  ISCA '86 Proceedings of the 13th annual international symposium on Computer architecture, pp.~28-37,
  1986.
  
\bibitem{mitchellga}
  Melanie Mitchell,
  \emph{An Introduction to Genetic Algorithms},
  Cambridge, MA: MIT Press,
  1996.

\bibitem{rlheli}
  Andrew Y. Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger and Eric Liang,
  \emph{Inverted autonomous helicopter flight via reinforcement learning},
  International Symposium on Experimental Robotics,
  \url{http://www.robotics.stanford.edu/~ang/papers/iser04-invertedflight.pdf},
  2004.
  
\bibitem{rlhci}
  S. Singh, D. Litman, M. Kearns and M. Walker,
  \emph{Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System},
  Journal of Artificial Intelligence Research (JAIR), Volume 16, pages 105-133, 2002,
  \url{http://web.eecs.umich.edu/~baveja/Papers/RLDSjair.pdf}.
  
\bibitem{evolutioningamedesign}
  Alex J. Champandard,
  \emph{Making Designers Obsolete? Evolution in Game Design},
  \url{http://aigamedev.com/open/interview/evolution-in-cityconquest/},
  2012.
  
\bibitem{blackandwhite}
  James Wexler,
  \emph{A look at the smarts behind Lionhead Studio?s ?Black and White? and where it can and will go in the future},
  University of Rochester, Rochester, NY 14627,
  \url{http://www.cs.rochester.edu/~brown/242/assts/termprojs/games.pdf},
  2002.
  
\bibitem{projectgothamracing}
  Thore Graepal (Ralf Herbrich, Mykel Kockenderfer, David Stern, Phil Trelford),
  \emph{Learning to Play: Machine Learning in Games},
  Applied Games Group, Microsoft Research Cambridge,
  \url{http://www.admin.cam.ac.uk/offices/research/documents/local/events/downloads/tm/06_ThoreGraepel.pdf}.
  
\bibitem{forza}
  \emph{Drivatar\texttrademark\ in Forza Motorsport},
  \url{http://research.microsoft.com/en-us/projects/drivatar/forza.aspx}.
  
\bibitem{2012the}
  Sergey Karakovskiy and Julian Togelius,
  \emph{Mario AI Benchmark and Competitions},
  \url{http://julian.togelius.com/Karakovskiy2012The.pdf},
  2012.
  
\bibitem{2010the}
  Julian Togelius, Sergey Karakovskiy and Robin Baumgarten,
  \emph{The 2009 Mario AI Competition},
  \url{http://julian.togelius.com/Togelius2010The.pdf},
  2010.
  
\bibitem{2014how}
  Julian Togelius,
  \emph{How to run a successful game-based AI competition},
  \url{http://julian.togelius.com/Togelius2014How.pdf},
  2014.
  
\bibitem{torcs}
  \emph{TORCS: The Open Racing Car Simulation},
  \url{http://torcs.sourceforge.net/}.

\bibitem{scrc}
  Daniele Loiacono, Pier Luca Lanzi, Julian Togelius, Enrique Onieva, David A. Pelta, Martin V. Butz, Thies D. Lnneker, Luigi Cardamone, Diego Perez, Yago Sez, Mike Preuss, and Jan Quadflieg,
  \emph{The 2009 Simulated Car Racing Championship},
  IEEE TRANSACTIONS ON COMPUTATIONAL INTELLIGENCE AND AI IN GAMES, VOL. 2, NO. 2,
  2010.
  
\bibitem{2kbot}
  \emph{The 2K BotPrize},
  \url{http://botprize.org/}
  
\bibitem{starcomp}
  Michael Buro, David Churchill,
  \emph{Real-Time Strategy Game Competitions},
  Association for the Advancement of Artificial Intelligence, AI Magazine, pp.~106-108,
  \url{https://skatgame.net/mburo/ps/aaai-competition-report-2012.pdf},
  2012.
  
\bibitem{imb}
  \emph{Infinite Mario Bros.},
  Created by Markus Perrson,
  \url{http://www.pcmariogames.com/infinite-mario.php}.
  
\bibitem{handa}
  H. Handa,
  \emph{Dimensionality reduction of scene and enemy information in mario},
  Proceedings of the IEEE Congress on Evolutionary Computation,  
  2011.
  
\bibitem{rossbagnell}
  S. Ross and J. A. Bagnell, 
  \emph{Efficient reductions for imitation learning},
  International Conference on Artificial Intelligence and Statistics (AISTATS),
  2010.
  
\bibitem{realm}
  Slawomir Bojarski and Clare Bates Congdon,
  \emph{REALM: A Rule-Based Evolutionary Computation Agent that Learns to Play Mario},
  2010 IEEE Conference on Computational Intelligence and Games (CIG?10) pp.~83-90.

\bibitem{gramev}
   D. Perez, M. Nicolau, M. O'Neill, and A. Brabazon,
   \emph{Evolving Behaviour Trees for the Mario AI Competition Using Grammatical Evolution},
   Proceedings of EvoApps, 2010, pp.~123?132.

\bibitem{feetsies}
  E. R. Speed, 
  \emph{Evolving a mario agent using cuckoo search and softmax heuristics},
  Proceedings of the IEEE Consumer Electronics Society?s Games Innovations Conference (ICE-GIC), 2010,
  pp.~1?7.
  
\bibitem{emapf}
  Thomas Willer Sandberg,
  \emph{Evolutionary Multi-Agent Potential Field based AI approach for SSC scenarios in RTS games},
   University of Copenhagen,
   2011.
  
\bibitem{ecj}
  \emph{ECJ: A Java-based Evolutionary Computation Research System},
  \url{https://cs.gmu.edu/~eclab/projects/ecj/}.
  
\bibitem{jgap}
  \emph{JGAP: Java Genetic Algorithms Package},
  \url{http://jgap.sourceforge.net/}.

\bibitem{astar}
  Robin Baumgarten,
  \emph{A* Mario Agent},
  \url{https://github.com/jumoel/mario-astar-robinbaumgarten}.
  
\bibitem{agentsmith}
  Ryan Small,
  \emph{Agent Smith: a Real-Time Game-Playing Agent for Interactive Dynamic Games},
  GECCO?08, July 12?16, 2008, Atlanta, Georgia, USA.
  \url{http://www.cs.bham.ac.uk/~wbl/biblio/gecco2008/docs/p1839.pdf}.
  

\end{thebibliography}
\end{document}

























